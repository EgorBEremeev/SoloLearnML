{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow import feature_column\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.utils as ku\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'C:\\git\\SoloLearnML\\stackoverflow\\TextDataset.csv'\n",
    "\n",
    "#it is just two column csv, like:\n",
    "# text;label\n",
    "# A wiki is run using wiki software;0\n",
    "# otherwise known as a wiki engine.;1\n",
    "\n",
    "dataframe = pd.read_csv(DATA_PATH, delimiter = ';')\n",
    "dataframe.head()\n",
    "\n",
    "# Or use this dummy dataframe\n",
    "# d = {'text': ['A wiki is run using wiki software',\n",
    "#               'otherwise known as a wiki engine.',\n",
    "#               'otherwise known as a wiki engine.',\n",
    "#               'A wiki engine is a type of content management system,',\n",
    "#               'but it differs from most other such systems,including blog software',\n",
    "#               'in that the content is created without any defined owner or leader',\n",
    "#               'and wikis have little inherent structure',\n",
    "#               'allowing structure to emerge according to the needs of the users',\n",
    "#               'There are dozens of different wiki engines in use',\n",
    "#               'both standalone and part of other software, such as bug tracking systems'\n",
    "#              ], \n",
    "#      'label': [1,0,1,1,1,0,0,1,0,0]}\n",
    "# dataframe = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing before feature_clolumn includes\n",
    "# - getting the vocabulary\n",
    "# - tokenization, which means only splitting on tokens. Encoding sentences with vocablary will be done by feature_column!\n",
    "# - padding\n",
    "# - truncating\n",
    "\n",
    "# Build vacabulary\n",
    "vocab_size = 100\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "sentences = dataframe['text'].to_list()\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=\"<OOV>\")\n",
    "\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# if word_index shorter then default value of vocab_size we'll save actual size\n",
    "vocab_size=len(word_index)\n",
    "print(\"vocab_size = word_index = \",len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentensec on tokens. here token = word\n",
    "# text_to_word_sequence() has good default filter for charachters include basic punctuation, tabs, and newlines\n",
    "dataframe['text'] = dataframe['text'].apply(tf.keras.preprocessing.text.text_to_word_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 6\n",
    "\n",
    "# paddind and trancating setnences\n",
    "# do that directly with strings without using tokenizer.texts_to_sequences()\n",
    "# the feature_colunm will convert strings into numbers\n",
    "dataframe['text']=dataframe['text'].apply(lambda x, N=max_length: (x + N * [''])[:N])\n",
    "dataframe['text']=dataframe['text'].apply(lambda x, N=max_length: x[:N])\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define method to create tf.data dataset from Pandas Dataframe\n",
    "def df_to_dataset(dataframe, label_column, shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    #labels = dataframe.pop(label_column)\n",
    "    labels = dataframe[label_column]\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe into train and validation sets\n",
    "train_df, val_df = train_test_split(dataframe, test_size=0.2)\n",
    "\n",
    "print(len(train_df), 'train examples')\n",
    "print(len(val_df), 'validation examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "ds = df_to_dataset(dataframe, 'label',shuffle=False,batch_size=batch_size)\n",
    "\n",
    "train_ds = df_to_dataset(train_df, 'label', batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val_df, 'label', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and small batch for demo\n",
    "example_batch = next(iter(ds))[0]\n",
    "example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to print exxample outputs of for defined feature_column\n",
    "\n",
    "def demo(feature_column):\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_column)\n",
    "    print(feature_layer(example_batch).numpy())\n",
    "    \n",
    "def seqdemo(feature_column):\n",
    "    feature_layer = tf.keras.experimental.SequenceFeatures(feature_column)\n",
    "    print(feature_layer(example_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical colunm for our text feature, which is preprocessed into lists of tokens\n",
    "# Note that key name should be the same as original column name in dataframe\n",
    "text_column = feature_column.sequence_categorical_column_with_vocabulary_list(key='text', \n",
    "                                                                     vocabulary_list=list(word_index))\n",
    "\n",
    "# arguemnt dimention here is exactly the dimension of the space in which tokens will be presented during model's learning\n",
    "# see the tutorial at https://www.tensorflow.org/beta/tutorials/text/word_embeddings\n",
    "text_embedding = feature_column.embedding_column(text_column, dimension=8)\n",
    "print(seqdemo(text_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The define the layers and model it self\n",
    "# This example uses Keras Functional API instead of Sequential just for more generallity\n",
    "\n",
    "# Define SequenceFeatures layer to pass feature_columns into Keras model\n",
    "sequence_feature_layer = tf.keras.experimental.SequenceFeatures(text_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs for each feature column. See\n",
    "# см. https://github.com/tensorflow/tensorflow/issues/27416#issuecomment-502218673\n",
    "feature_layer_inputs = {}\n",
    "sequence_feature_layer_inputs = {}\n",
    "\n",
    "# Here we have just one column\n",
    "\n",
    "sequence_feature_layer_inputs['text'] = tf.keras.Input(shape=(max_length,), name='text', dtype=tf.string)\n",
    "print(sequence_feature_layer_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_feature_layer(sequence_feature_layer_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_feature_layer_outputs, _ = sequence_feature_layer(sequence_feature_layer_inputs)\n",
    "print(sequence_feature_layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define outputs of SequenceFeatures layer \n",
    "# And accually use them as first layer of the model\n",
    "\n",
    "# note here that SequenceFeatures layer produce tuple of two tensors as output. We need just first to pass next.\n",
    "sequence_feature_layer_outputs, _ = sequence_feature_layer(sequence_feature_layer_inputs)\n",
    "print(sequence_feature_layer_outputs)\n",
    "# Add consequences layers. See https://keras.io/getting-started/functional-api-guide/\n",
    "\n",
    "# Conv1D and MaxPooling1D will learn features from words order\n",
    "x = tf.keras.layers.Conv1D(8,4)(sequence_feature_layer_outputs)\n",
    "x = tf.keras.layers.MaxPooling1D(2)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x= tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "# This example supposes binary classification, as labels are 0 or 1\n",
    "x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[v for v in sequence_feature_layer_inputs.values()], outputs=x)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# This example supposes binary classification, as labels are 0 or 1\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              #run_eagerly=True\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that fit() method looking up features in train_ds and valdation_ds by name in \n",
    "# tf.keras.Input(shape=(max_length,), name='text'\n",
    "\n",
    "# This model of cause will learn nothing because of fake data.\n",
    "\n",
    "num_epochs = 5\n",
    "history = model.fit(train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=1\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
